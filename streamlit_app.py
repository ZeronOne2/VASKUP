"""
Patent Analysis RAG System
íŠ¹í—ˆ ë¶„ì„ì„ ìœ„í•œ Advanced Agentic RAG ì‹œìŠ¤í…œ

This Streamlit application provides an intelligent patent analysis system
that uses RAG (Retrieval-Augmented Generation) technology to analyze patents
and answer questions about them.
"""

import streamlit as st
import pandas as pd
import os
from pathlib import Path
import re
from datetime import datetime

# Vector Store related imports (only when needed)
try:
    from src.vector_store.patent_vector_store import PatentVectorStore
    from src.vector_store.embedding_manager import EmbeddingManager
    from src.patent_search.patent_parser import PatentDataProcessor
except ImportError as e:
    print(f"Vector Store ê´€ë ¨ ëª¨ë“ˆ import ì‹¤íŒ¨: {e}")
    PatentVectorStore = None
    EmbeddingManager = None
    PatentDataProcessor = None

# Set page configuration
st.set_page_config(
    page_title="íŠ¹í—ˆ ë¶„ì„ RAG ì‹œìŠ¤í…œ",
    page_icon="ğŸ“„",
    layout="wide",
    initial_sidebar_state="expanded",
)


def main():
    """Main application function"""

    # Title and description
    st.title("ğŸ“„ íŠ¹í—ˆ ë¶„ì„ RAG ì‹œìŠ¤í…œ")
    st.markdown(
        """
    **Advanced Agentic RAG ê¸°ë°˜ íŠ¹í—ˆ ë¶„ì„ ë„êµ¬**
    
    ì´ ì‹œìŠ¤í…œì€ ì—‘ì…€ íŒŒì¼ì— í¬í•¨ëœ íŠ¹í—ˆ ë²ˆí˜¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íŠ¹í—ˆ ì •ë³´ë¥¼ ìë™ ìˆ˜ì§‘í•˜ê³ , 
    AIë¥¼ í™œìš©í•˜ì—¬ íŠ¹í—ˆì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•´ë“œë¦½ë‹ˆë‹¤.
    """
    )

    # Sidebar for navigation
    with st.sidebar:
        st.header("ğŸ§­ ë©”ë‰´")

        # Check if environment is properly set up
        env_status = check_environment()

        if env_status:
            st.success("âœ… í™˜ê²½ ì„¤ì • ì™„ë£Œ")

            page = st.selectbox(
                "í˜ì´ì§€ ì„ íƒ",
                [
                    "ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ",
                    "ğŸ” íŠ¹í—ˆ ê²€ìƒ‰",
                    "ğŸ§ª Vector Store í…ŒìŠ¤íŠ¸",
                    "â“ ì§ˆë¬¸ ê´€ë¦¬",
                    "ğŸš€ ê³ ë„í™”ëœ CRAG",
                    "ğŸ“Š ê²°ê³¼ ë³´ê¸°",
                    "âš™ï¸ ì„¤ì •",
                ],
            )
        else:
            st.error("âŒ í™˜ê²½ ì„¤ì • í•„ìš”")
            st.info("API í‚¤ë¥¼ .env íŒŒì¼ì— ì„¤ì •í•´ì£¼ì„¸ìš”.")
            page = "âš™ï¸ ì„¤ì •"

    # Main content area
    if page == "ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ":
        show_file_upload_page()
    elif page == "ğŸ” íŠ¹í—ˆ ê²€ìƒ‰":
        show_patent_search_page()
    elif page == "ğŸ§ª Vector Store í…ŒìŠ¤íŠ¸":
        show_vector_store_test_page()
    elif page == "â“ ì§ˆë¬¸ ê´€ë¦¬":
        show_question_management_page()
    elif page == "ğŸš€ ê³ ë„í™”ëœ CRAG":
        show_langgraph_crag_page()
    elif page == "ğŸ“Š ê²°ê³¼ ë³´ê¸°":
        show_results_page()
    elif page == "âš™ï¸ ì„¤ì •":
        show_settings_page()


def check_environment():
    """Check if the environment is properly configured"""
    required_env_vars = ["SERPAPI_API_KEY", "OPENAI_API_KEY"]

    # Load environment variables
    from dotenv import load_dotenv

    load_dotenv()

    missing_vars = []
    for var in required_env_vars:
        if not os.getenv(var) or os.getenv(var) == f"your_{var.lower()}_here":
            missing_vars.append(var)

    return len(missing_vars) == 0


def detect_patent_column(df):
    """
    íŠ¹í—ˆ ë²ˆí˜¸ê°€ í¬í•¨ëœ ì»¬ëŸ¼ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ëŠ” í•¨ìˆ˜
    """
    # íŠ¹í—ˆ ë²ˆí˜¸ ê´€ë ¨ í‚¤ì›Œë“œë“¤
    patent_keywords = [
        "patent",
        "patents",
        "íŠ¹í—ˆ",
        "íŠ¹í—ˆë²ˆí˜¸",
        "íŠ¹í—ˆ_ë²ˆí˜¸",
        "patent_number",
        "patent_no",
        "patentno",
        "number",
        "no",
        "ë²ˆí˜¸",
        "id",
        "patent_id",
    ]

    # ì»¬ëŸ¼ëª…ìœ¼ë¡œ ê²€ìƒ‰
    for col in df.columns:
        col_lower = str(col).lower().strip()
        for keyword in patent_keywords:
            if keyword.lower() in col_lower:
                return col

    # ì»¬ëŸ¼ëª…ìœ¼ë¡œ ì°¾ì§€ ëª»í•œ ê²½ìš°, ë°ì´í„° íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰
    for col in df.columns:
        # ìƒ˜í”Œ ë°ì´í„°ì—ì„œ íŠ¹í—ˆ ë²ˆí˜¸ íŒ¨í„´ì„ ì°¾ì•„ë´„
        sample_data = df[col].dropna().astype(str).head(10)
        patent_pattern_count = 0

        for value in sample_data:
            if is_valid_patent_number(value):
                patent_pattern_count += 1

        # ìƒ˜í”Œì˜ 70% ì´ìƒì´ íŠ¹í—ˆ ë²ˆí˜¸ íŒ¨í„´ì´ë©´ í•´ë‹¹ ì»¬ëŸ¼ìœ¼ë¡œ íŒë‹¨
        if patent_pattern_count >= len(sample_data) * 0.7:
            return col

    return None


def is_valid_patent_number(patent_str):
    """
    SerpAPI Google Patents API í˜•ì‹ì— ë§ëŠ” íŠ¹í—ˆ ë²ˆí˜¸ì¸ì§€ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜
    í˜•ì‹: <country_code><patent_number><classification> (ì˜ˆ: US11734097B1, KR10-2021-0123456, EP3456789A1)
    """
    if not isinstance(patent_str, str):
        patent_str = str(patent_str)

    patent_str = patent_str.strip().upper()

    # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ 'INVALID_PATENT' ê°™ì€ ëª…ì‹œì ìœ¼ë¡œ ì˜ëª»ëœ ê°’ ì²´í¬
    if not patent_str or patent_str.lower() in ["invalid_patent", "n/a", "na", "-", ""]:
        return False

    # SerpAPI Google Patents APIì—ì„œ ì§€ì›í•˜ëŠ” íŠ¹í—ˆ ë²ˆí˜¸ íŒ¨í„´
    # íŒ¨í„´: <êµ­ê°€ì½”ë“œ><íŠ¹í—ˆë²ˆí˜¸><ë¶„ë¥˜ê¸°í˜¸(ì„ íƒì )>
    patterns = [
        # US íŠ¹í—ˆ: US + ìˆ«ì + ë¬¸ì+ìˆ«ì ì¡°í•© (ì˜ˆ: US11734097B1, US10123456A1)
        r"^US\d{7,10}[A-Z]?\d*$",
        # KR íŠ¹í—ˆ: KR + ìˆ«ì ë˜ëŠ” KR + ìˆ«ì-ìˆ«ì-ìˆ«ì (ì˜ˆ: KR1234567890, KR10-2021-0123456)
        r"^KR\d{7,12}$",
        r"^KR\d{2}-\d{4}-\d{7}$",
        # EP íŠ¹í—ˆ: EP + ìˆ«ì + ë¬¸ì+ìˆ«ì ì¡°í•© (ì˜ˆ: EP3456789A1, EP1234567B1)
        r"^EP\d{6,10}[A-Z]?\d*$",
        # JP íŠ¹í—ˆ: JP + ìˆ«ì + ë¬¸ì+ìˆ«ì ì¡°í•© (ì˜ˆ: JP2023123456A, JP6789012B2)
        r"^JP\d{6,12}[A-Z]?\d*$",
        # CN íŠ¹í—ˆ: CN + ìˆ«ì + ë¬¸ì+ìˆ«ì ì¡°í•© (ì˜ˆ: CN202310987654A, CN112345678B)
        r"^CN\d{8,15}[A-Z]?\d*$",
        # ê¸°íƒ€ êµ­ê°€ ì½”ë“œ íŒ¨í„´ (2ê¸€ì êµ­ê°€ì½”ë“œ + ìˆ«ì + ì„ íƒì  ë¬¸ì)
        r"^[A-Z]{2}\d{6,12}[A-Z]?\d*$",
    ]

    for pattern in patterns:
        if re.match(pattern, patent_str):
            return True

    return False


def validate_patent_data(df, patent_column):
    """
    íŠ¹í—ˆ ë°ì´í„°ì˜ ìœ íš¨ì„±ì„ ê²€ì¦í•˜ëŠ” í•¨ìˆ˜
    """
    if patent_column not in df.columns:
        return False, f"ì„ íƒëœ ì»¬ëŸ¼ '{patent_column}'ì´ ë°ì´í„°ì— ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    patent_data = df[patent_column].dropna()

    if len(patent_data) == 0:
        return False, f"ì»¬ëŸ¼ '{patent_column}'ì— ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    # íŠ¹í—ˆ ë²ˆí˜¸ í˜•ì‹ ê²€ì¦
    valid_patents = 0
    total_patents = len(patent_data)

    for patent in patent_data:
        if is_valid_patent_number(str(patent)):
            valid_patents += 1

    validity_ratio = valid_patents / total_patents

    if validity_ratio < 0.5:  # 50% ë¯¸ë§Œì´ ìœ íš¨í•œ íŠ¹í—ˆ ë²ˆí˜¸
        return (
            False,
            f"ì»¬ëŸ¼ '{patent_column}'ì—ì„œ ìœ íš¨í•œ íŠ¹í—ˆ ë²ˆí˜¸ í˜•ì‹ì´ {validity_ratio:.1%}ë§Œ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ì»¬ëŸ¼ì„ ì„ íƒí–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.",
        )

    return (
        True,
        f"ìœ íš¨ì„± ê²€ì¦ ì™„ë£Œ: {valid_patents}/{total_patents} ({validity_ratio:.1%}) ê°œì˜ ìœ íš¨í•œ íŠ¹í—ˆ ë²ˆí˜¸ ë°œê²¬",
    )


def load_file_safely(uploaded_file):
    """
    íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        file_extension = uploaded_file.name.split(".")[-1].lower()

        if file_extension == "csv":
            # CSV íŒŒì¼ ì¸ì½”ë”© ìë™ ê°ì§€
            try:
                df = pd.read_csv(uploaded_file, encoding="utf-8")
            except UnicodeDecodeError:
                try:
                    uploaded_file.seek(0)  # íŒŒì¼ í¬ì¸í„° ë¦¬ì…‹
                    df = pd.read_csv(uploaded_file, encoding="cp949")
                except:
                    uploaded_file.seek(0)
                    df = pd.read_csv(uploaded_file, encoding="latin-1")

        elif file_extension in ["xlsx", "xls"]:
            df = pd.read_excel(uploaded_file)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {file_extension}")

        # ê¸°ë³¸ì ì¸ ë°ì´í„° ê²€ì¦
        if df.empty:
            raise ValueError("ì—…ë¡œë“œëœ íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        if len(df.columns) == 0:
            raise ValueError("íŒŒì¼ì— ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

        return df, None

    except Exception as e:
        return None, str(e)


def show_file_upload_page():
    """File upload page with enhanced functionality"""
    st.header("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ")

    st.markdown(
        """
    íŠ¹í—ˆ ë²ˆí˜¸ê°€ í¬í•¨ëœ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.
    
    **ì§€ì› í˜•ì‹:**
    - ğŸ“Š Excel íŒŒì¼: `.xlsx`, `.xls`
    - ğŸ“„ CSV íŒŒì¼: `.csv`
    
    **ìš”êµ¬ì‚¬í•­:**
    - íŠ¹í—ˆ ë²ˆí˜¸ê°€ í¬í•¨ëœ ì—´ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
    - ì§€ì›ë˜ëŠ” íŠ¹í—ˆ ë²ˆí˜¸ í˜•ì‹: US1234567, KR1234567, EP1234567 ë“±
    """
    )

    uploaded_file = st.file_uploader(
        "íŒŒì¼ ì„ íƒ",
        type=["xlsx", "xls", "csv"],
        help="íŠ¹í—ˆ ë²ˆí˜¸ê°€ í¬í•¨ëœ Excel ë˜ëŠ” CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.",
    )

    if uploaded_file is not None:
        with st.spinner("íŒŒì¼ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘..."):
            # íŒŒì¼ ë¡œë“œ
            df, error = load_file_safely(uploaded_file)

            if error:
                st.error(f"âŒ íŒŒì¼ ë¡œë“œ ì˜¤ë¥˜: {error}")
                st.markdown(
                    """
                **í•´ê²° ë°©ë²•:**
                - íŒŒì¼ì´ ì†ìƒë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
                - Excel íŒŒì¼ì˜ ê²½ìš° ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì—ì„œ ì—´ë ¤ìˆì§€ ì•Šì€ì§€ í™•ì¸í•˜ì„¸ìš”
                - CSV íŒŒì¼ì˜ ê²½ìš° UTF-8 ì¸ì½”ë”©ìœ¼ë¡œ ì €ì¥ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
                """
                )
                return

        st.success(f"âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_file.name}")

        # íŒŒì¼ ì •ë³´ í‘œì‹œ
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ì´ í–‰ ìˆ˜", f"{df.shape[0]:,}")
        with col2:
            st.metric("ì´ ì—´ ìˆ˜", f"{df.shape[1]:,}")
        with col3:
            file_size = uploaded_file.size / 1024  # KB
            if file_size > 1024:
                st.metric("íŒŒì¼ í¬ê¸°", f"{file_size/1024:.1f} MB")
            else:
                st.metric("íŒŒì¼ í¬ê¸°", f"{file_size:.1f} KB")

        # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (PyArrow ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•œ ë°ì´í„° íƒ€ì… ì •ë¦¬)
        st.subheader("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
        preview_df = df.head().copy()
        # ëª¨ë“  ì»¬ëŸ¼ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ PyArrow í˜¸í™˜ì„± í™•ë³´
        for col in preview_df.columns:
            preview_df[col] = preview_df[col].astype(str)
        st.dataframe(preview_df, use_container_width=True)

        # ìë™ ì»¬ëŸ¼ ê°ì§€
        st.subheader("ğŸ¯ íŠ¹í—ˆ ë²ˆí˜¸ ì»¬ëŸ¼ ì„ íƒ")

        detected_column = detect_patent_column(df)

        if detected_column:
            st.success(f"ğŸ” ìë™ ê°ì§€ëœ íŠ¹í—ˆ ë²ˆí˜¸ ì»¬ëŸ¼: **{detected_column}**")
            default_index = list(df.columns).index(detected_column)
        else:
            st.info(
                "ğŸ” íŠ¹í—ˆ ë²ˆí˜¸ ì»¬ëŸ¼ì„ ìë™ìœ¼ë¡œ ê°ì§€í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ ì„ íƒí•´ì£¼ì„¸ìš”."
            )
            default_index = 0

        patent_column = st.selectbox(
            "íŠ¹í—ˆ ë²ˆí˜¸ê°€ í¬í•¨ëœ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”:",
            df.columns.tolist(),
            index=default_index,
            help="íŠ¹í—ˆ ë²ˆí˜¸ê°€ í¬í•¨ëœ ì»¬ëŸ¼ì„ ì„ íƒí•˜ì„¸ìš”. ìë™ ê°ì§€ê°€ í‹€ë ¸ë‹¤ë©´ ì˜¬ë°”ë¥¸ ì»¬ëŸ¼ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        )

        if patent_column:
            # ì„ íƒëœ ì»¬ëŸ¼ì˜ ë°ì´í„° ê²€ì¦
            is_valid, validation_message = validate_patent_data(df, patent_column)

            if is_valid:
                st.success(f"âœ… {validation_message}")
            else:
                st.error(f"âŒ {validation_message}")
                st.markdown(
                    """
                **í•´ê²° ë°©ë²•:**
                - ë‹¤ë¥¸ ì»¬ëŸ¼ì„ ì„ íƒí•´ë³´ì„¸ìš”
                - íŠ¹í—ˆ ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥¸ í˜•ì‹ì¸ì§€ í™•ì¸í•˜ì„¸ìš” (ì˜ˆ: US1234567, KR1234567)
                - íŠ¹í—ˆ ë²ˆí˜¸ì— ë¶ˆí•„ìš”í•œ ê³µë°±ì´ë‚˜ íŠ¹ìˆ˜ë¬¸ìê°€ ì—†ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
                """
                )

            # ìƒ˜í”Œ ë°ì´í„° í‘œì‹œ
            st.write(f"**ì„ íƒëœ ì»¬ëŸ¼:** {patent_column}")

            sample_data = df[patent_column].dropna().head(10)

            col1, col2 = st.columns(2)
            with col1:
                st.write("**ìƒ˜í”Œ ë°ì´í„°:**")
                for i, value in enumerate(sample_data, 1):
                    validity_icon = "âœ…" if is_valid_patent_number(str(value)) else "âŒ"
                    st.write(f"{i}. {validity_icon} {value}")

            with col2:
                # í†µê³„ ì •ë³´
                total_count = len(df[patent_column].dropna())
                valid_count = sum(
                    1
                    for x in df[patent_column].dropna()
                    if is_valid_patent_number(str(x))
                )

                st.write("**ë°ì´í„° í†µê³„:**")
                st.write(f"- ì´ íŠ¹í—ˆ ìˆ˜: {total_count:,}")
                st.write(f"- ìœ íš¨í•œ íŠ¹í—ˆ: {valid_count:,}")
                st.write(f"- ìœ íš¨ì„± ë¹„ìœ¨: {valid_count/total_count*100:.1f}%")

            # ë°ì´í„° ì €ì¥ ë²„íŠ¼
            if st.button("ğŸ’¾ ë°ì´í„° ì €ì¥", type="primary", disabled=not is_valid):
                if is_valid:
                    try:
                        # ë°ì´í„° ì €ì¥
                        data_dir = Path("data")
                        data_dir.mkdir(exist_ok=True)

                        # ì›ë³¸ íŒŒì¼ëª… ê¸°ë°˜ìœ¼ë¡œ ì €ì¥
                        original_name = uploaded_file.name.split(".")[0]
                        file_path = data_dir / f"uploaded_patents_{original_name}.xlsx"
                        df.to_excel(file_path, index=False)

                        # ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                        st.session_state["patent_data"] = df
                        st.session_state["patent_column"] = patent_column
                        st.session_state["data_file_path"] = str(file_path)
                        st.session_state["upload_stats"] = {
                            "total_patents": total_count,
                            "valid_patents": valid_count,
                            "file_name": uploaded_file.name,
                        }

                        st.success("âœ… ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.balloons()  # ì¶•í•˜ ì• ë‹ˆë©”ì´ì…˜

                        st.info(
                            "ğŸš€ ì´ì œ 'ğŸ” íŠ¹í—ˆ ê²€ìƒ‰' í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ íŠ¹í—ˆ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”."
                        )

                        # ìë™ìœ¼ë¡œ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™í•˜ëŠ” ì˜µì…˜
                        if st.button("ğŸ” íŠ¹í—ˆ ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™"):
                            st.session_state.selected_page = "ğŸ” íŠ¹í—ˆ ê²€ìƒ‰"
                            st.rerun()

                    except Exception as e:
                        st.error(f"âŒ ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                else:
                    st.error("âŒ ìœ íš¨í•˜ì§€ ì•Šì€ ë°ì´í„°ëŠ” ì €ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def show_patent_search_page():
    """Patent search page with SerpAPI integration"""
    st.header("ğŸ” íŠ¹í—ˆ ê²€ìƒ‰")

    if "patent_data" not in st.session_state:
        st.warning("âš ï¸ ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.info("'ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ' í˜ì´ì§€ì—ì„œ íŠ¹í—ˆ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
        return

    # SerpAPI í´ë¼ì´ì–¸íŠ¸ì™€ ì›¹ ìŠ¤í¬ë˜í•‘ ëª¨ë“ˆ import
    try:
        from src.patent_search.serp_client import SerpPatentClient, PatentSearchError
        from src.patent_search.description_scraper import (
            GooglePatentsDescriptionScraper,
            ScrapingError,
        )
    except ImportError:
        st.error(
            "âŒ íŠ¹í—ˆ ê²€ìƒ‰ ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
        )
        return

    st.success("âœ… ì—…ë¡œë“œëœ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤.")

    df = st.session_state["patent_data"]
    patent_column = st.session_state["patent_column"]

    # ì—…ë¡œë“œ í†µê³„ê°€ ìˆìœ¼ë©´ í‘œì‹œ
    if "upload_stats" in st.session_state:
        stats = st.session_state["upload_stats"]

        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("ì´ íŠ¹í—ˆ ìˆ˜", f"{stats['total_patents']:,}")
        with col2:
            st.metric("ìœ íš¨í•œ íŠ¹í—ˆ", f"{stats['valid_patents']:,}")
        with col3:
            st.metric(
                "ìœ íš¨ì„± ë¹„ìœ¨",
                f"{stats['valid_patents']/stats['total_patents']*100:.1f}%",
            )
        with col4:
            st.metric("ì›ë³¸ íŒŒì¼", stats["file_name"])

    st.write(f"**íŠ¹í—ˆ ë²ˆí˜¸ ì»¬ëŸ¼:** {patent_column}")

    # API í‚¤ í™•ì¸
    serpapi_key = os.getenv("SERPAPI_API_KEY")
    if not serpapi_key or serpapi_key.startswith("your_"):
        st.error("âŒ SERPAPI_API_KEYê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        st.info("'âš™ï¸ ì„¤ì •' í˜ì´ì§€ì—ì„œ API í‚¤ë¥¼ í™•ì¸í•˜ê³  ì„¤ì •í•˜ì„¸ìš”.")
        return

    # ê²€ìƒ‰ ì˜µì…˜
    st.subheader("ğŸ”§ ê²€ìƒ‰ ì„¤ì •")

    col1, col2, col3 = st.columns(3)
    with col1:
        search_mode = st.selectbox(
            "ê²€ìƒ‰ ëª¨ë“œ ì„ íƒ:",
            ["í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ (1ê°œ)", "ë¶€ë¶„ ê²€ìƒ‰ (ìµœëŒ€ 5ê°œ)", "ì „ì²´ ê²€ìƒ‰"],
            help="í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ìœ¼ë¡œ ë¨¼ì € API ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”.",
        )

    with col2:
        country_code = st.selectbox(
            "ì–¸ì–´ ì½”ë“œ:", ["en", "ko", "ja", "zh"], help="ê²€ìƒ‰ ê²°ê³¼ ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”."
        )

    with col3:
        enable_web_scraping = st.checkbox(
            "ğŸ•·ï¸ ì›¹ ìŠ¤í¬ë˜í•‘ í™œì„±í™”",
            value=False,
            help="Google Patentsì—ì„œ ìƒì„¸í•œ íŠ¹í—ˆ ì„¤ëª…ì„ ì¶”ê°€ë¡œ ìŠ¤í¬ë˜í•‘í•©ë‹ˆë‹¤. (ì²˜ë¦¬ ì‹œê°„ì´ ê¸¸ì–´ì§‘ë‹ˆë‹¤)",
        )

    # ìƒ˜í”Œ íŠ¹í—ˆ ë²ˆí˜¸ í‘œì‹œ
    st.subheader("ğŸ“‹ ê²€ìƒ‰ ëŒ€ìƒ íŠ¹í—ˆ ë²ˆí˜¸")
    valid_patents = []
    invalid_patents = []

    all_patents = df[patent_column].dropna().tolist()
    for patent in all_patents:
        if is_valid_patent_number(str(patent)):
            valid_patents.append(str(patent))
        else:
            invalid_patents.append(str(patent))

    # ê²€ìƒ‰í•  íŠ¹í—ˆ ìˆ˜ ê²°ì •
    if search_mode == "í…ŒìŠ¤íŠ¸ ê²€ìƒ‰ (1ê°œ)":
        search_patents = valid_patents[:1]
    elif search_mode == "ë¶€ë¶„ ê²€ìƒ‰ (ìµœëŒ€ 5ê°œ)":
        search_patents = valid_patents[:5]
    else:
        search_patents = valid_patents

    col1, col2 = st.columns([2, 1])
    with col1:
        st.write(f"**ê²€ìƒ‰í•  íŠ¹í—ˆ ({len(search_patents)}ê°œ):**")
        for i, patent in enumerate(search_patents[:10], 1):  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
            st.write(f"{i}. âœ… {patent}")

        if len(search_patents) > 10:
            st.write(f"... ì™¸ {len(search_patents) - 10}ê°œ")

    with col2:
        st.write("**í†µê³„:**")
        st.write(f"- ì´ íŠ¹í—ˆ: {len(all_patents):,}")
        st.write(f"- ìœ íš¨í•œ íŠ¹í—ˆ: {len(valid_patents):,}")
        st.write(f"- ë¬´íš¨í•œ íŠ¹í—ˆ: {len(invalid_patents):,}")
        st.write(f"- ê²€ìƒ‰ ì˜ˆì •: {len(search_patents):,}")

    # ê²€ìƒ‰ ì‹¤í–‰ ë²„íŠ¼
    if st.button(
        "ğŸš€ íŠ¹í—ˆ ê²€ìƒ‰ ì‹œì‘", type="primary", disabled=len(search_patents) == 0
    ):
        if len(search_patents) == 0:
            st.error("âŒ ê²€ìƒ‰í•  ìœ íš¨í•œ íŠ¹í—ˆê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # ê²€ìƒ‰ ì‹¤í–‰
        progress_bar = st.progress(0)
        status_text = st.empty()
        results_container = st.container()

        def progress_callback(current, total, patent, success=True, error=None):
            progress = current / total
            progress_bar.progress(progress)

            if success:
                status_text.text(f"ì§„í–‰ ì¤‘... {current}/{total} - {patent} âœ…")
            else:
                status_text.text(
                    f"ì§„í–‰ ì¤‘... {current}/{total} - {patent} âŒ ({error})"
                )

        try:
            with st.spinner("íŠ¹í—ˆ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
                client = SerpPatentClient(serpapi_key)

                # ë°°ì¹˜ ê²€ìƒ‰ ì‹¤í–‰
                search_results = client.batch_search_patents(
                    search_patents,
                    country_code=country_code,
                    progress_callback=progress_callback,
                )

                # ì›¹ ìŠ¤í¬ë˜í•‘ ì¶”ê°€ ì²˜ë¦¬ (ì„ íƒì‚¬í•­)
                if enable_web_scraping and search_results["summary"]["success"] > 0:
                    status_text.text("ğŸ•·ï¸ ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ìƒì„¸ ì„¤ëª… ìˆ˜ì§‘ ì¤‘...")

                    # Description Link ê¸°ë°˜ ì›¹ ìŠ¤í¬ë˜í•‘ URL ìˆ˜ì§‘ (ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ description_link ìš°ì„  ì‚¬ìš©)
                    patent_urls = []
                    url_to_patent_mapping = {}  # URLê³¼ íŠ¹í—ˆ ID ë§¤í•‘ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬

                    for patent_id, result in search_results["results"].items():
                        # ìš”êµ¬ì‚¬í•­: description_linkë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©
                        if "description_link" in result and result["description_link"]:
                            url = result["description_link"]
                            patent_urls.append(url)
                            url_to_patent_mapping[url] = patent_id
                        elif (
                            "google_patents_url" in result
                            and result["google_patents_url"]
                        ):
                            # fallback: google_patents_url ì‚¬ìš©
                            url = result["google_patents_url"]
                            patent_urls.append(url)
                            url_to_patent_mapping[url] = patent_id
                        else:
                            # ë§ˆì§€ë§‰ fallback: patent_idë¡œë¶€í„° Google Patents URL ìƒì„±
                            base_url = "https://patents.google.com/patent/"
                            url = f"{base_url}{patent_id}/en"
                            patent_urls.append(url)
                            url_to_patent_mapping[url] = patent_id

                    if patent_urls:
                        # ì›¹ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
                        scraper = GooglePatentsDescriptionScraper(
                            requests_per_second=1.0,  # ì•ˆì „í•œ ì†ë„
                            timeout=30,
                            max_retries=2,
                        )

                        def scraping_progress_callback(completed, total, result):
                            progress = (len(search_patents) + completed) / (
                                len(search_patents) + total
                            )
                            progress_bar.progress(progress)

                            status = "âœ… ì„±ê³µ" if result.success else "âŒ ì‹¤íŒ¨"
                            status_text.text(
                                f"ğŸ•·ï¸ ì›¹ ìŠ¤í¬ë˜í•‘ {completed}/{total} - {result.patent_id}: {status}"
                            )

                        scraping_results = scraper.batch_scrape_patents(
                            patent_urls,
                            max_workers=2,
                            progress_callback=scraping_progress_callback,
                        )

                        # ìŠ¤í¬ë˜í•‘ ê²°ê³¼ë¥¼ ê²€ìƒ‰ ê²°ê³¼ì— í†µí•© (URL ê¸°ë°˜ ë§¤ì¹­)
                        scraping_success = 0
                        for scraping_result in scraping_results:
                            if scraping_result.success:
                                # URL ê¸°ë°˜ìœ¼ë¡œ íŠ¹í—ˆ ID ì°¾ê¸°
                                matched_patent_id = url_to_patent_mapping.get(
                                    scraping_result.url
                                )

                                if (
                                    matched_patent_id
                                    and matched_patent_id in search_results["results"]
                                ):
                                    search_result = search_results["results"][
                                        matched_patent_id
                                    ]
                                    search_result["scraped_title"] = (
                                        scraping_result.title
                                    )
                                    search_result["scraped_abstract"] = (
                                        scraping_result.abstract
                                    )
                                    search_result["scraped_description"] = (
                                        scraping_result.description
                                    )
                                    search_result["scraped_claims"] = (
                                        scraping_result.claims
                                    )
                                    search_result["scraping_success"] = True
                                    scraping_success += 1

                        # ìŠ¤í¬ë˜í•‘ í†µê³„ ì—…ë°ì´íŠ¸
                        search_results["scraping_summary"] = {
                            "total_urls": len(patent_urls),
                            "successful_scraping": scraping_success,
                            "scraping_rate": (
                                scraping_success / len(patent_urls) * 100
                                if patent_urls
                                else 0
                            ),
                        }

                        status_text.text(
                            f"âœ… ì›¹ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {scraping_success}/{len(patent_urls)} ì„±ê³µ"
                        )

                # ê²°ê³¼ ì €ì¥
                st.session_state["search_results"] = search_results
                st.session_state["last_search_timestamp"] = datetime.now().isoformat()
                st.session_state["web_scraping_enabled"] = enable_web_scraping

                # ê²°ê³¼ í‘œì‹œ
                progress_bar.empty()
                status_text.empty()

                # ê²€ìƒ‰ ìš”ì•½
                summary = search_results["summary"]

                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("ì´ ê²€ìƒ‰", f"{summary['total']:,}")
                with col2:
                    st.metric("ì„±ê³µ", f"{summary['success']:,}")
                with col3:
                    st.metric("ì‹¤íŒ¨", f"{summary['failed']:,}")
                with col4:
                    st.metric("ì„±ê³µë¥ ", f"{summary['success_rate']:.1f}%")

                # ì›¹ ìŠ¤í¬ë˜í•‘ í†µê³„ í‘œì‹œ (ìˆëŠ” ê²½ìš°)
                if "scraping_summary" in search_results:
                    st.subheader("ğŸ•·ï¸ ì›¹ ìŠ¤í¬ë˜í•‘ ê²°ê³¼")
                    scraping_summary = search_results["scraping_summary"]

                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric(
                            "ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ", f"{scraping_summary['total_urls']:,}"
                        )
                    with col2:
                        st.metric(
                            "ìŠ¤í¬ë˜í•‘ ì„±ê³µ",
                            f"{scraping_summary['successful_scraping']:,}",
                        )
                    with col3:
                        st.metric(
                            "ìŠ¤í¬ë˜í•‘ ì„±ê³µë¥ ",
                            f"{scraping_summary['scraping_rate']:.1f}%",
                        )

                if summary["success"] > 0:
                    st.success(f"âœ… {summary['success']}ê°œ íŠ¹í—ˆ ê²€ìƒ‰ ì™„ë£Œ!")
                    st.info("ğŸ” 'ğŸ“Š ê²°ê³¼ ë³´ê¸°' í˜ì´ì§€ì—ì„œ ìƒì„¸ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

                    # ì²« ë²ˆì§¸ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°
                    if search_results["results"]:
                        st.subheader("ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°")
                        first_patent = list(search_results["results"].keys())[0]
                        first_result = search_results["results"][first_patent]

                        with st.expander(
                            f"ğŸ” {first_patent} - {first_result['title'][:50]}..."
                        ):
                            col1, col2 = st.columns(2)
                            with col1:
                                st.write("**ê¸°ë³¸ ì •ë³´:**")
                                st.write(f"- ì œëª©: {first_result['title']}")
                                st.write(
                                    f"- ë°œëª…ì: {', '.join(first_result['inventor'][:3])}"
                                )
                                st.write(
                                    f"- ì¶œì›ì¼: {first_result['application_date']}"
                                )
                                st.write(
                                    f"- ê³µê°œì¼: {first_result['publication_date']}"
                                )

                            with col2:
                                st.write("**ìš”ì•½:**")
                                # ì›¹ ìŠ¤í¬ë˜í•‘ëœ ì´ˆë¡ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ê¸°ë³¸ ì´ˆë¡ ì‚¬ìš©
                                if first_result.get(
                                    "scraping_success"
                                ) and first_result.get("scraped_abstract"):
                                    abstract = first_result["scraped_abstract"][:300]
                                    st.write("ğŸ•·ï¸ *ì›¹ ìŠ¤í¬ë˜í•‘ëœ ìƒì„¸ ì´ˆë¡:*")
                                else:
                                    abstract = first_result["abstract"][:300]

                                if len(abstract) > 300:
                                    abstract += "..."
                                st.write(abstract)

                        # ì›¹ ìŠ¤í¬ë˜í•‘ ì„±ê³µ ì‹œ ìƒì„¸ ì •ë³´ í‘œì‹œ (ì „ì²´ ë„ˆë¹„ ì‚¬ìš©)
                        if first_result.get("scraping_success"):
                            st.write("---")
                            st.write("### ğŸ•·ï¸ ì›¹ ìŠ¤í¬ë˜í•‘ëœ ìƒì„¸ ì •ë³´")

                            # ìŠ¤í¬ë˜í•‘ ì†ŒìŠ¤ í‘œì‹œ
                            if (
                                "description_link" in first_result
                                and first_result["description_link"]
                            ):
                                st.info(
                                    "ğŸ“¡ **ìŠ¤í¬ë˜í•‘ ì†ŒìŠ¤**: SerpAPI Description Link (ìš”êµ¬ì‚¬í•­ ì¤€ìˆ˜)"
                                )
                            else:
                                st.info("ğŸ“¡ **ìŠ¤í¬ë˜í•‘ ì†ŒìŠ¤**: Google Patents ì›¹í˜ì´ì§€")

                            # ìƒì„¸ ì„¤ëª… ë¯¸ë¦¬ë³´ê¸°
                            if first_result.get("scraped_description"):
                                desc = first_result["scraped_description"]
                                desc_length = len(desc)
                                st.write(f"**ğŸ“‹ ìƒì„¸ ì„¤ëª…** ({desc_length:,} ë¬¸ì):")

                                # ì²˜ìŒ 500ì í‘œì‹œ
                                preview_desc = (
                                    desc[:500] + "..." if len(desc) > 500 else desc
                                )
                                with st.expander("ìƒì„¸ ì„¤ëª… ë¯¸ë¦¬ë³´ê¸°", expanded=True):
                                    st.text_area(
                                        "", preview_desc, height=150, disabled=True
                                    )
                            else:
                                st.warning("ğŸ“‹ ìƒì„¸ ì„¤ëª…ì„ ìŠ¤í¬ë˜í•‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

                            # ì²­êµ¬í•­ ë¯¸ë¦¬ë³´ê¸° (SerpAPI ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜´)
                            claims = first_result.get("claims", [])
                            if claims and len(claims) > 0:
                                claims_count = len(claims)
                                st.write(f"**âš–ï¸ ì²­êµ¬í•­** ({claims_count}ê°œ):")
                                st.info("ğŸ’¡ ì²­êµ¬í•­ì€ SerpAPI ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.")

                                with st.expander("ì²­êµ¬í•­ ë¯¸ë¦¬ë³´ê¸°", expanded=True):
                                    # ì²« 3ê°œ ì²­êµ¬í•­ í‘œì‹œ
                                    for i, claim in enumerate(claims[:3]):
                                        st.write(f"**ì²­êµ¬í•­ {i+1}:**")
                                        claim_preview = (
                                            claim[:300] + "..."
                                            if len(claim) > 300
                                            else claim
                                        )
                                        st.write(claim_preview)
                                        if i < min(2, claims_count - 1):
                                            st.write("---")

                                    if claims_count > 3:
                                        st.info(
                                            f"+ {claims_count - 3}ê°œ ì²­êµ¬í•­ ë” ìˆìŒ"
                                        )
                            else:
                                st.warning("âŒ ì²­êµ¬í•­ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

                if summary["failed"] > 0:
                    st.warning(f"âš ï¸ {summary['failed']}ê°œ íŠ¹í—ˆ ê²€ìƒ‰ ì‹¤íŒ¨")

                    with st.expander("ì‹¤íŒ¨í•œ íŠ¹í—ˆ ëª©ë¡ ë³´ê¸°"):
                        for patent, error in search_results["errors"].items():
                            st.write(f"âŒ {patent}: {error}")

        except PatentSearchError as e:
            st.error(f"âŒ íŠ¹í—ˆ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        except Exception as e:
            st.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")

    # ì´ì „ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš° í‘œì‹œ
    if "search_results" in st.session_state:
        st.subheader("ğŸ“Š ìµœê·¼ ê²€ìƒ‰ ê²°ê³¼")
        last_results = st.session_state["search_results"]
        last_timestamp = st.session_state.get("last_search_timestamp", "ì•Œ ìˆ˜ ì—†ìŒ")

        st.info(f"ë§ˆì§€ë§‰ ê²€ìƒ‰: {last_timestamp}")

        summary = last_results["summary"]
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("ê²€ìƒ‰í•œ íŠ¹í—ˆ", f"{summary['total']:,}")
        with col2:
            st.metric("ì„±ê³µí•œ íŠ¹í—ˆ", f"{summary['success']:,}")
        with col3:
            st.metric("ì„±ê³µë¥ ", f"{summary['success_rate']:.1f}%")

        if st.button("ğŸ“Š ìƒì„¸ ê²°ê³¼ ë³´ê¸°"):
            st.session_state.selected_page = "ğŸ“Š ê²°ê³¼ ë³´ê¸°"
            st.rerun()


def show_vector_store_test_page():
    """Vector Store í…ŒìŠ¤íŠ¸ í˜ì´ì§€"""
    st.header("ğŸ§ª Vector Store í…ŒìŠ¤íŠ¸")
    st.markdown("Task 5-6ì—ì„œ êµ¬í˜„ëœ Vector Store ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•´ë³´ì„¸ìš”!")
    st.markdown(
        "**ê¸°ì¡´ ì›Œí¬í”Œë¡œìš°**: ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ â†’ ğŸ” íŠ¹í—ˆ ê²€ìƒ‰ â†’ ğŸ“„ ì›¹ ìŠ¤í¬ë˜í•‘ â†’ âš™ï¸ ì²­í‚¹ â†’ ğŸ’¾ Vector Store ì €ì¥"
    )

    # Import Vector Store modules
    try:
        from src.patent_search.patent_parser import PatentParser, PatentDataProcessor
        from src.vector_store.patent_vector_store import PatentVectorStore
        from src.vector_store.embedding_manager import EmbeddingManager
        import time
    except ImportError as e:
        st.error(f"âŒ Vector Store ëª¨ë“ˆì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        st.info("Vector Store ê´€ë ¨ ëª¨ë“ˆì´ ì˜¬ë°”ë¥´ê²Œ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # Initialize session state for vector store
    if "vector_store" not in st.session_state:
        st.session_state.vector_store = None
    if "processed_patents" not in st.session_state:
        st.session_state.processed_patents = []

    # Load Vector Store function (without cache for debugging)
    def load_vector_store():
        """Vector Storeë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        try:
            # í•„ìˆ˜ í´ë˜ìŠ¤ë“¤ì´ importë˜ì—ˆëŠ”ì§€ í™•ì¸
            if PatentVectorStore is None:
                st.error("PatentVectorStore í´ë˜ìŠ¤ë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None

            # Vector Store ì´ˆê¸°í™”
            vector_store = PatentVectorStore(
                collection_name="patent_chunks",
                persist_directory="./chroma_db",
                reset_collection=False,
                enable_preprocessing=True,
                enable_performance_monitoring=True,
            )
            return vector_store
        except Exception as e:
            st.error(f"Vector Store ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback

            st.text(traceback.format_exc())
            return None

    # ìë™ Vector Store ì´ˆê¸°í™” ì‹œë„ (í•¨ìˆ˜ ì •ì˜ í›„)
    if st.session_state.vector_store is None:
        try:
            st.session_state.vector_store = load_vector_store()
        except Exception as e:
            print(f"ìë™ Vector Store ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            # ì˜¤ë¥˜ ì‹œ Noneìœ¼ë¡œ ìœ ì§€

    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.subheader("âš™ï¸ Vector Store ì„¤ì •")

        # Vector Store ìƒíƒœ í‘œì‹œ
        if st.session_state.vector_store:
            st.success("âœ… Vector Store ì¤€ë¹„ë¨")

            # Vector Store í†µê³„
            try:
                stats = st.session_state.vector_store.get_stats()
                st.metric("ì €ì¥ëœ ì²­í¬ ìˆ˜", stats.get("total_chunks", 0))
                st.metric("ê³ ìœ  íŠ¹í—ˆ ìˆ˜", stats.get("unique_patents", 0))
            except:
                pass
        else:
            st.warning("âŒ Vector Store ë¯¸ì´ˆê¸°í™”")

        # Vector Store ì´ˆê¸°í™”/ì¬ì´ˆê¸°í™”
        button_text = (
            "ğŸ”„ Vector Store ì¬ì´ˆê¸°í™”"
            if st.session_state.vector_store
            else "ğŸš€ Vector Store ì´ˆê¸°í™”"
        )
        if st.button(button_text):
            with st.spinner("Vector Store ì´ˆê¸°í™” ì¤‘..."):
                st.session_state.vector_store = load_vector_store()
                if st.session_state.vector_store:
                    st.success("Vector Store ì´ˆê¸°í™” ì™„ë£Œ!")
                    st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
                else:
                    st.error("Vector Store ì´ˆê¸°í™” ì‹¤íŒ¨!")

    # íƒ­ ìƒì„±
    tab1, tab2, tab3 = st.tabs(
        ["ğŸ“„ ê¸°ì¡´ ë°ì´í„° ì €ì¥", "ğŸ¯ ìœ ì‚¬ë„ ê²€ìƒ‰", "ğŸ“Š Vector Store ê´€ë¦¬"]
    )

    with tab1:
        st.subheader("ê¸°ì¡´ íŠ¹í—ˆ ê²€ìƒ‰ ê²°ê³¼ë¥¼ Vector Storeì— ì €ì¥")
        st.info("ğŸ” íŠ¹í—ˆ ê²€ìƒ‰ í˜ì´ì§€ì—ì„œ ì²˜ë¦¬ëœ ê²°ê³¼ë¥¼ Vector Storeì— ì €ì¥í•©ë‹ˆë‹¤.")

        # ê²€ìƒ‰ ê²°ê³¼ í™•ì¸
        if "search_results" in st.session_state:
            search_results = st.session_state.search_results
            st.success("âœ… ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆìŠµë‹ˆë‹¤!")

            col1, col2 = st.columns(2)
            with col1:
                search_count = len(search_results.get("results", {}))
                st.metric("ê²€ìƒ‰ëœ íŠ¹í—ˆ ìˆ˜", search_count)
            with col2:
                # ì›¹ ìŠ¤í¬ë˜í•‘ëœ íŠ¹í—ˆ ìˆ˜ ê³„ì‚°
                scraping_count = 0
                for result in search_results.get("results", {}).values():
                    if result.get("scraping_success", False):
                        scraping_count += 1
                st.metric("ìŠ¤í¬ë˜í•‘ëœ íŠ¹í—ˆ ìˆ˜", scraping_count)

            if st.button(
                "ğŸ’¾ ê¸°ì¡´ ê²€ìƒ‰ ê²°ê³¼ë¥¼ Vector Storeì— ì €ì¥",
                disabled=not st.session_state.vector_store,
            ):
                with st.spinner("ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ ì¤‘..."):
                    try:
                        # í•„ìˆ˜ í´ë˜ìŠ¤ í™•ì¸
                        if PatentDataProcessor is None:
                            st.error(
                                "PatentDataProcessor í´ë˜ìŠ¤ë¥¼ importí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                            )
                            return

                        # ë°ì´í„° ì²˜ë¦¬ - search_resultsì—ì„œ ìŠ¤í¬ë˜í•‘ ë°ì´í„° ì¶”ì¶œ
                        processor = PatentDataProcessor()

                        # ìŠ¤í¬ë˜í•‘ ê²°ê³¼ë¥¼ ë³„ë„ êµ¬ì¡°ë¡œ ìƒì„±
                        scraping_results = {"results": []}
                        for patent_id, result in search_results.get(
                            "results", {}
                        ).items():
                            if result.get("scraping_success", False):
                                scraping_result = {
                                    "patent_id": patent_id,
                                    "url": result.get("description_link", ""),
                                    "title": result.get(
                                        "scraped_title", result.get("title", "")
                                    ),
                                    "abstract": result.get(
                                        "scraped_abstract", result.get("abstract", "")
                                    ),
                                    "description": result.get(
                                        "scraped_description", ""
                                    ),
                                    "claims": result.get(
                                        "scraped_claims", result.get("claims", [])
                                    ),
                                    "success": True,
                                }
                                scraping_results["results"].append(scraping_result)

                        patents, chunks = processor.process_search_results(
                            search_results,
                            scraping_results,
                        )

                        if patents and chunks:
                            st.success(
                                f"âœ… {len(patents)}ê°œ íŠ¹í—ˆ, {len(chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ!"
                            )

                            # Vector Storeì— ì €ì¥
                            success_count, error_count = (
                                st.session_state.vector_store.add_document_chunks_batch(
                                    chunks
                                )
                            )

                            if success_count > 0:
                                st.success(
                                    f"ğŸ‰ {success_count}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ! (ì˜¤ë¥˜: {error_count}ê°œ)"
                                )
                                st.session_state.processed_patents.extend(patents)

                                # ê²°ê³¼ í‘œì‹œ
                                with st.expander("ğŸ“‹ ì²˜ë¦¬ëœ íŠ¹í—ˆ ëª©ë¡", expanded=True):
                                    for i, patent in enumerate(patents, 1):
                                        st.markdown(f"**{i}. {patent.title}**")
                                        st.markdown(
                                            f"íŠ¹í—ˆë²ˆí˜¸: `{patent.patent_number}`"
                                        )
                                        st.markdown(
                                            f"ì¶œì›ì¼: {patent.filing_date or 'N/A'}"
                                        )
                                        st.markdown(
                                            f"ì²­í¬ ìˆ˜: {len([c for c in chunks if c.patent_number == patent.patent_number])}"
                                        )
                                        st.markdown("---")
                            else:
                                st.error(f"âŒ ì €ì¥ ì‹¤íŒ¨! ì˜¤ë¥˜: {error_count}ê°œ")
                        else:
                            st.warning("ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

                    except Exception as e:
                        st.error(f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        else:
            st.warning("âš ï¸ ì €ì¥í•  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.info(
                "ë¨¼ì € 'ğŸ” íŠ¹í—ˆ ê²€ìƒ‰' í˜ì´ì§€ì—ì„œ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•˜ê³  ìŠ¤í¬ë˜í•‘ì„ ì™„ë£Œí•´ì£¼ì„¸ìš”."
            )

        # ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ì˜µì…˜
        st.markdown("---")
        st.subheader("ğŸ“ ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©")

        # ê¸°ì¡´ ìƒ˜í”Œ ë°ì´í„° íŒŒì¼ í™•ì¸
        import json
        from pathlib import Path

        sample_files = []
        for file_path in [
            "test_data.json",
            "sample_patent_data.json",
            "data/test_patents.json",
        ]:
            if Path(file_path).exists():
                sample_files.append(file_path)

        if sample_files:
            selected_file = st.selectbox("ìƒ˜í”Œ ë°ì´í„° íŒŒì¼ ì„ íƒ:", sample_files)

            if st.button("ğŸ“‚ ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ë° ì €ì¥"):
                try:
                    with open(selected_file, "r", encoding="utf-8") as f:
                        sample_data = json.load(f)

                    # ë°ì´í„° í˜•ì‹ì— ë”°ë¼ ì²˜ë¦¬
                    if isinstance(sample_data, list) and len(sample_data) > 0:
                        # Patent ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
                        from src.patent_search.patent_parser import Patent

                        patents = []
                        for patent_data in sample_data:
                            if isinstance(patent_data, dict):
                                patent = Patent(
                                    patent_number=patent_data.get(
                                        "patent_number", "UNKNOWN"
                                    ),
                                    title=patent_data.get("title", "ì œëª© ì—†ìŒ"),
                                    abstract=patent_data.get("abstract", ""),
                                    description_html=patent_data.get(
                                        "description_html", ""
                                    ),
                                    claims=patent_data.get("claims", []),
                                    filing_date=patent_data.get("filing_date"),
                                    inventors=patent_data.get("inventors", []),
                                    assignees=patent_data.get("assignees", []),
                                )
                                patents.append(patent)

                        if patents:
                            # ì²­í‚¹ ì²˜ë¦¬
                            processor = PatentDataProcessor()
                            chunks = processor.create_chunks_from_patents(patents)

                            if chunks:
                                st.success(
                                    f"âœ… {len(patents)}ê°œ íŠ¹í—ˆ, {len(chunks)}ê°œ ì²­í¬ ë¡œë“œ ì™„ë£Œ!"
                                )

                                # Vector Storeì— ì €ì¥
                                success_count, error_count = (
                                    st.session_state.vector_store.add_document_chunks_batch(
                                        chunks
                                    )
                                )

                                if success_count > 0:
                                    st.success(
                                        f"ğŸ‰ {success_count}ê°œ ì²­í¬ ì €ì¥ ì™„ë£Œ! (ì˜¤ë¥˜: {error_count}ê°œ)"
                                    )
                                else:
                                    st.error(f"âŒ ì €ì¥ ì‹¤íŒ¨! ì˜¤ë¥˜: {error_count}ê°œ")
                            else:
                                st.warning("ì²­í‚¹ ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                        else:
                            st.warning("ìœ íš¨í•œ íŠ¹í—ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        st.warning("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹ì…ë‹ˆë‹¤.")

                except Exception as e:
                    st.error(f"ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        else:
            st.info("ì‚¬ìš© ê°€ëŠ¥í•œ ìƒ˜í”Œ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    with tab2:
        st.subheader("Vector Store ìœ ì‚¬ë„ ê²€ìƒ‰")

        if not st.session_state.vector_store:
            st.warning("ë¨¼ì € Vector Storeë¥¼ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
            return

        col1, col2 = st.columns([3, 1])
        with col1:
            similarity_query = st.text_input(
                "ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
                placeholder="ì˜ˆ: deep learning algorithm, neural network",
                help="ì €ì¥ëœ íŠ¹í—ˆ ì²­í¬ì—ì„œ ìœ ì‚¬í•œ ë‚´ìš©ì„ ì°¾ìŠµë‹ˆë‹¤.",
            )
        with col2:
            search_limit = st.selectbox("ê²°ê³¼ ê°œìˆ˜:", [5, 10, 20], index=1)

        if st.button("ğŸ¯ ìœ ì‚¬ë„ ê²€ìƒ‰"):
            if similarity_query:
                with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                    start_time = time.time()
                    results = st.session_state.vector_store.search_similar(
                        similarity_query, n_results=search_limit, include_distances=True
                    )
                    search_time = time.time() - start_time

                st.success(
                    f"âœ… ê²€ìƒ‰ ì™„ë£Œ! ({search_time:.3f}ì´ˆ, {results['total_results']}ê°œ ê²°ê³¼)"
                )

                if results["results"]:
                    for i, result in enumerate(results["results"], 1):
                        with st.expander(
                            f"ğŸ“„ ê²°ê³¼ {i} (ìœ ì‚¬ë„: {result.get('similarity', 0):.3f})",
                            expanded=i <= 3,
                        ):
                            st.markdown(f"**ì²­í¬ ID:** `{result['chunk_id']}`")

                            # ë©”íƒ€ë°ì´í„° í™•ì¸ ë° ë””ë²„ê¹…
                            metadata = result.get("metadata", {})

                            # ë‹¤ì–‘í•œ ê°€ëŠ¥í•œ í•„ë“œëª…ë“¤ì„ ì‹œë„
                            patent_number = (
                                metadata.get("patent_number")
                                or metadata.get("patent_id")
                                or metadata.get("id")
                                or "N/A"
                            )

                            section = (
                                metadata.get("section_type")
                                or metadata.get("section")
                                or metadata.get("chunk_type")
                                or metadata.get("type")
                                or "N/A"
                            )

                            st.markdown(f"**íŠ¹í—ˆë²ˆí˜¸:** {patent_number}")
                            st.markdown(f"**ì„¹ì…˜:** {section}")
                            st.markdown(
                                f"**ê±°ë¦¬:** {result.get('distance', 'N/A'):.4f}"
                            )

                            # ë©”íƒ€ë°ì´í„° ìƒì„¸ ì •ë³´ í‘œì‹œ (ì²˜ìŒ 10ê°œ ê²°ê³¼)
                            if i <= 10:
                                st.markdown("**ğŸ“‹ ë©”íƒ€ë°ì´í„° ìƒì„¸:**")
                                st.json(metadata)

                            st.markdown("**ë‚´ìš©:**")
                            st.markdown(f"```\n{result['content'][:500]}...\n```")
                else:
                    st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.warning("ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

    with tab3:
        st.subheader("Vector Store ê´€ë¦¬")

        if not st.session_state.vector_store:
            st.warning("ë¨¼ì € Vector Storeë¥¼ ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”.")
            return

        # í†µê³„ ì •ë³´
        try:
            stats = st.session_state.vector_store.get_stats()

            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ì²­í¬ ìˆ˜", stats.get("total_chunks", 0))
            with col2:
                st.metric("ê³ ìœ  íŠ¹í—ˆ ìˆ˜", stats.get("unique_patents", 0))
            with col3:
                st.metric(
                    "ì»¬ë ‰ì…˜ í¬ê¸° (MB)", f"{stats.get('collection_size_mb', 0):.2f}"
                )

            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
            if (
                hasattr(st.session_state.vector_store, "performance_monitor")
                and st.session_state.vector_store.performance_monitor
            ):
                st.subheader("ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§")
                monitor = st.session_state.vector_store.performance_monitor

                if monitor.operations:
                    performance_stats = monitor.get_stats()

                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric(
                            "ì´ ì‘ì—… ìˆ˜", performance_stats.get("total_operations", 0)
                        )
                    with col2:
                        st.metric(
                            "ì„±ê³µë¥ ", f"{performance_stats.get('success_rate', 0):.1f}%"
                        )
                    with col3:
                        st.metric(
                            "í‰ê·  ì‘ë‹µì‹œê°„",
                            f"{performance_stats.get('avg_duration', 0):.3f}ì´ˆ",
                        )

                    # ì‘ì—…ë³„ í†µê³„
                    op_breakdown = performance_stats.get("operation_breakdown", {})
                    if op_breakdown:
                        st.subheader("ì‘ì—…ë³„ í†µê³„")
                        for op_type, op_stats in op_breakdown.items():
                            with st.expander(f"{op_type} í†µê³„"):
                                st.json(op_stats)
                else:
                    st.info("ì•„ì§ ì„±ëŠ¥ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ê´€ë¦¬ ì‘ì—…
            st.subheader("ğŸ› ï¸ ê´€ë¦¬ ì‘ì—…")

            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸ”„ Vector Store ì¬ì„¤ì •"):
                    if st.session_state.vector_store:
                        st.session_state.vector_store.reset_collection()
                        st.success("Vector Storeê°€ ì¬ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        st.rerun()

            with col2:
                if st.button("ğŸ“Š ìƒì„¸ í†µê³„ ë³´ê¸°"):
                    st.json(stats)

        except Exception as e:
            st.error(f"í†µê³„ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")


def show_question_management_page():
    """Question management page"""
    st.header("â“ ì§ˆë¬¸ ê´€ë¦¬")

    st.markdown(
        """
    íŠ¹í—ˆ ë¶„ì„ì„ ìœ„í•œ ì§ˆë¬¸ì„ ê´€ë¦¬í•˜ì„¸ìš”.
    ê° íŠ¹í—ˆì— ëŒ€í•´ ë™ì¼í•œ ì§ˆë¬¸ë“¤ì´ ì ìš©ë©ë‹ˆë‹¤.
    """
    )

    # Default questions
    default_questions = [
        "ì´ íŠ¹í—ˆì˜ ì£¼ìš” ê¸°ìˆ ì  íŠ¹ì§•ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì´ íŠ¹í—ˆì˜ ì‘ìš© ë¶„ì•¼ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
        "ì´ íŠ¹í—ˆì˜ í˜ì‹ ì„±ì€ ë¬´ì—‡ì¸ê°€ìš”?",
        "ì´ íŠ¹í—ˆì˜ ìƒì—…ì  ê°€ì¹˜ëŠ” ì–´ë–»ê²Œ í‰ê°€ë˜ë‚˜ìš”?",
    ]

    # Initialize questions in session state
    if "custom_questions" not in st.session_state:
        st.session_state["custom_questions"] = default_questions.copy()

    st.subheader("ğŸ“ í˜„ì¬ ì§ˆë¬¸ ëª©ë¡")

    # Display current questions
    for i, question in enumerate(st.session_state["custom_questions"]):
        col1, col2 = st.columns([4, 1])
        with col1:
            st.write(f"{i+1}. {question}")
        with col2:
            if st.button("ğŸ—‘ï¸", key=f"delete_{i}", help="ì§ˆë¬¸ ì‚­ì œ"):
                st.session_state["custom_questions"].pop(i)
                st.rerun()

    # Add new question
    st.subheader("â• ìƒˆ ì§ˆë¬¸ ì¶”ê°€")
    new_question = st.text_input("ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:")

    if st.button("ì§ˆë¬¸ ì¶”ê°€") and new_question:
        st.session_state["custom_questions"].append(new_question)
        st.success("âœ… ì§ˆë¬¸ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.rerun()

    # Reset to defaults
    if st.button("ğŸ”„ ê¸°ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ì´ˆê¸°í™”"):
        st.session_state["custom_questions"] = default_questions.copy()
        st.success("âœ… ê¸°ë³¸ ì§ˆë¬¸ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.rerun()


def show_rag_analysis_page():
    """RAG analysis page"""
    st.header("ğŸ¤– RAG ë¶„ì„")
    st.info("ğŸš§ RAG ë¶„ì„ ê¸°ëŠ¥ì€ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ êµ¬í˜„ë©ë‹ˆë‹¤.")


def show_results_page():
    """Results page"""
    st.header("ğŸ“Š ê²°ê³¼ ë³´ê¸°")
    st.info("ğŸš§ ê²°ê³¼ ë³´ê¸° ê¸°ëŠ¥ì€ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ êµ¬í˜„ë©ë‹ˆë‹¤.")


def show_settings_page():
    """Settings page"""
    st.header("âš™ï¸ ì„¤ì •")

    st.markdown(
        """
    ### í™˜ê²½ ì„¤ì • í™•ì¸
    
    API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”:
    """
    )

    # Environment variables check
    from dotenv import load_dotenv

    load_dotenv()

    env_vars = {
        "SERPAPI_API_KEY": "SerpAPI (íŠ¹í—ˆ ê²€ìƒ‰ìš©)",
        "OPENAI_API_KEY": "OpenAI (AI ë¶„ì„ìš©)",
    }

    for var, description in env_vars.items():
        value = os.getenv(var)
        if value and value != f"your_{var.lower()}_here":
            st.success(f"âœ… {description}: ì„¤ì •ë¨")
        else:
            st.error(f"âŒ {description}: ì„¤ì • í•„ìš”")

    st.markdown(
        """
    ### ì„¤ì • ë°©ë²•
    
    1. í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ì˜ `.env` íŒŒì¼ì„ í¸ì§‘í•˜ì„¸ìš”.
    2. ê° API í‚¤ë¥¼ ì‹¤ì œ ê°’ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”:
    
    ```
    SERPAPI_API_KEY=your_actual_serpapi_key
    OPENAI_API_KEY=your_actual_openai_key
    ```
    
    3. íŒŒì¼ì„ ì €ì¥í•˜ê³  ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¬ì‹œì‘í•˜ì„¸ìš”.
    """
    )

    # System information
    st.subheader("ğŸ’» ì‹œìŠ¤í…œ ì •ë³´")
    st.write(f"**Python ë²„ì „:** {st.__version__}")
    st.write(f"**ì‘ì—… ë””ë ‰í† ë¦¬:** {os.getcwd()}")


def show_langgraph_crag_page():
    """LangGraph ê¸°ë°˜ Corrective RAG (CRAG) ë¶„ì„ í˜ì´ì§€ (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + ì¬ë­í‚¹)"""
    st.title("ğŸš€ ê³ ë„í™”ëœ LangGraph CRAG ë¶„ì„")
    st.markdown("**í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ + Cross-encoder ì¬ë­í‚¹ ê¸°ë°˜ ê³ ê¸‰ RAG íŒŒì´í”„ë¼ì¸**")

    # CRAG Pipeline import (ì§€ì—° ë¡œë”©)
    try:
        from src.langgraph_crag.crag_pipeline import CorrectiveRAGPipeline

        crag_available = True
    except ImportError as e:
        st.error(f"LangGraph CRAG ëª¨ë“ˆì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        crag_available = False
        return

    # Vector Store í™•ì¸
    if not st.session_state.get("vector_store"):
        st.warning(
            "ğŸ” Vector Storeê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Vector Store í…ŒìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ë¨¼ì € ì´ˆê¸°í™”í•´ì£¼ì„¸ìš”."
        )
        return

    # Pipeline Configuration
    st.sidebar.header("ğŸ”§ ê³ ë„í™”ëœ CRAG ì„¤ì •")

    with st.sidebar.expander("ëª¨ë¸ ì„¤ì •", expanded=True):
        model_name = st.selectbox(
            "LLM ëª¨ë¸",
            ["gpt-4o-mini", "gpt-3.5-turbo", "gpt-4", "gpt-4-turbo"],
            index=0,
        )

    with st.sidebar.expander("ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„¤ì •", expanded=True):
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì •
        enable_hybrid_search = st.checkbox(
            "ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í™œì„±í™”",
            value=True,
            help="Vector Search + BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°í•©",
        )

        if enable_hybrid_search:
            hybrid_vector_weight = st.slider(
                "Vector Search ê°€ì¤‘ì¹˜",
                min_value=0.1,
                max_value=0.9,
                value=0.6,
                step=0.1,
                help="í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì—ì„œ Vector Search ë¹„ì¤‘ (ë‚˜ë¨¸ì§€ëŠ” BM25)",
            )
        else:
            hybrid_vector_weight = 0.6

        # ì¬ë­í‚¹ ì„¤ì •
        enable_reranking = st.checkbox(
            "ğŸ¯ Jina AI ì¬ë­í‚¹ í™œì„±í™”",
            value=True,
            help="Jina AIì˜ ê³ ì„±ëŠ¥ rerankerë¥¼ í†µí•œ ì •ë°€í•œ ì§ˆë¬¸-ë¬¸ì„œ ê´€ë ¨ì„± ì¬í‰ê°€",
        )

        if enable_reranking:
            rerank_top_k = st.slider(
                "ì¬ë­í‚¹ ëŒ€ìƒ ë¬¸ì„œ ìˆ˜",
                min_value=5,
                max_value=20,
                value=10,
                step=1,
                help="ìƒìœ„ ëª‡ ê°œ ë¬¸ì„œë¥¼ ì¬ë­í‚¹í• ì§€ ì„¤ì •",
            )

            # Jina ëª¨ë¸ ì„ íƒ
            jina_model = st.selectbox(
                "Jina Reranker ëª¨ë¸",
                [
                    "jina-reranker-v2-base-multilingual",
                    "jina-colbert-v2",
                    "jina-reranker-v1-base-en",
                    "jina-reranker-v1-tiny-en",
                ],
                index=0,
                help="ì‚¬ìš©í•  Jina reranker ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
            )
        else:
            rerank_top_k = 10
            jina_model = "jina-reranker-v2-base-multilingual"

    with st.sidebar.expander("íŒŒì´í”„ë¼ì¸ ì„¤ì •", expanded=True):
        max_retries = st.slider("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜", 1, 5, 2)

    with st.sidebar.expander("API í‚¤ í™•ì¸", expanded=False):
        openai_key_status = "âœ… ì„¤ì •ë¨" if os.getenv("OPENAI_API_KEY") else "âŒ í•„ìš”"
        jina_key_status = "âœ… ì„¤ì •ë¨" if os.getenv("JINA_API_KEY") else "âŒ í•„ìš”"

        st.markdown(f"**OpenAI API:** {openai_key_status}")
        st.markdown(f"**Jina AI API:** {jina_key_status}")

        # ê²€ìƒ‰ ì‹œìŠ¤í…œ ì •ë³´ í‘œì‹œ
        search_info = []
        if enable_hybrid_search:
            search_info.append("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Vector + BM25)")
        else:
            search_info.append("ğŸ“Š Vector Store ê²€ìƒ‰")

        if enable_reranking:
            search_info.append("ğŸ¯ Jina AI ì¬ë­í‚¹")

        st.info(f"ğŸ’¡ ê²€ìƒ‰ ëª¨ë“œ: {' + '.join(search_info)}")

        if not os.getenv("OPENAI_API_KEY"):
            st.error("OpenAI API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
            return

        if enable_reranking and not os.getenv("JINA_API_KEY"):
            st.error("Jina AI ì¬ë­í‚¹ì„ ìœ„í•´ì„œëŠ” Jina API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
            return

    # Initialize pipeline
    pipeline_config_changed = (
        st.session_state.get("last_hybrid_search") != enable_hybrid_search
        or st.session_state.get("last_reranking") != enable_reranking
        or st.session_state.get("last_vector_weight") != hybrid_vector_weight
        or st.session_state.get("last_rerank_top_k") != rerank_top_k
        or st.session_state.get("last_jina_model") != jina_model
    )

    if (
        "crag_pipeline" not in st.session_state
        or pipeline_config_changed
        or st.button("ğŸ”„ CRAG Pipeline ì¬ì´ˆê¸°í™”")
    ):

        try:
            with st.spinner("ê³ ë„í™”ëœ CRAG Pipeline ì´ˆê¸°í™” ì¤‘..."):
                st.session_state.crag_pipeline = CorrectiveRAGPipeline(
                    vector_store=st.session_state.vector_store,
                    model_name=model_name,
                    max_retries=max_retries,
                    enable_hybrid_search=enable_hybrid_search,
                    enable_reranking=enable_reranking,
                    hybrid_vector_weight=hybrid_vector_weight,
                    rerank_top_k=rerank_top_k,
                    jina_api_key=os.getenv("JINA_API_KEY"),
                    jina_model=jina_model,
                )

                # ì„¤ì • ì €ì¥
                st.session_state.last_hybrid_search = enable_hybrid_search
                st.session_state.last_reranking = enable_reranking
                st.session_state.last_vector_weight = hybrid_vector_weight
                st.session_state.last_rerank_top_k = rerank_top_k
                st.session_state.last_jina_model = jina_model

            st.success("âœ… ê³ ë„í™”ëœ LangGraph CRAG Pipeline ì´ˆê¸°í™” ì™„ë£Œ!")
        except Exception as e:
            st.error(f"Pipeline ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            st.exception(e)
            return

    # Pipeline Info
    if st.session_state.get("crag_pipeline"):
        with st.expander("ğŸ“Š ê³ ë„í™”ëœ Pipeline ì •ë³´", expanded=False):
            pipeline_info = st.session_state.crag_pipeline.get_pipeline_info()

            col1, col2 = st.columns(2)
            with col1:
                st.markdown("**íŒŒì´í”„ë¼ì¸ ìœ í˜•:**")
                st.code(pipeline_info["pipeline_type"])
                st.markdown("**ëª¨ë¸:**")
                st.code(pipeline_info["model_name"])
                st.markdown("**ìµœëŒ€ ì¬ì‹œë„:**")
                st.code(pipeline_info["max_retries"])

            with col2:
                st.markdown("**ì£¼ìš” ê¸°ëŠ¥:**")
                for feature in pipeline_info["features"]:
                    st.markdown(f"â€¢ {feature}")

            st.markdown("**ì›Œí¬í”Œë¡œìš°:**")
            for step in pipeline_info["workflow"]:
                st.markdown(f"â€¢ {step}")

            # ê²€ìƒ‰ ì„¤ì • ì •ë³´ ì¶”ê°€
            if "search_configuration" in pipeline_info:
                st.markdown("---")
                st.markdown("**ğŸ” ê²€ìƒ‰ ì‹œìŠ¤í…œ ì„¤ì •:**")
                search_config = pipeline_info["search_configuration"]

                config_col1, config_col2 = st.columns(2)
                with config_col1:
                    hybrid_status = (
                        "âœ… í™œì„±í™”"
                        if search_config.get("hybrid_search_enabled")
                        else "âŒ ë¹„í™œì„±í™”"
                    )
                    st.markdown(f"â€¢ **í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰:** {hybrid_status}")

                    if search_config.get("hybrid_search_enabled"):
                        vector_weight = search_config.get("hybrid_vector_weight", 0.6)
                        bm25_weight = search_config.get("hybrid_bm25_weight", 0.4)
                        st.markdown(f"  - Vector ê°€ì¤‘ì¹˜: {vector_weight:.1f}")
                        st.markdown(f"  - BM25 ê°€ì¤‘ì¹˜: {bm25_weight:.1f}")

                with config_col2:
                    rerank_status = (
                        "âœ… í™œì„±í™”"
                        if search_config.get("reranking_enabled")
                        else "âŒ ë¹„í™œì„±í™”"
                    )
                    st.markdown(f"â€¢ **Cross-encoder ì¬ë­í‚¹:** {rerank_status}")

                    if search_config.get("reranking_enabled"):
                        rerank_model = search_config.get("reranker_model", "N/A")
                        rerank_loaded = search_config.get("reranker_loaded", False)
                        st.markdown(f"  - ëª¨ë¸: {rerank_model}")
                        st.markdown(f"  - ë¡œë”© ìƒíƒœ: {'âœ…' if rerank_loaded else 'â³'}")
                        st.markdown(
                            f"  - ì¬ë­í‚¹ ëŒ€ìƒ: ìƒìœ„ {search_config.get('rerank_top_k', 10)}ê°œ"
                        )

    # Main Query Interface
    st.header("ğŸ’¬ íŠ¹í—ˆ ì§ˆì˜")

    # Pre-defined example queries
    example_queries = [
        "ë¬´ì„ ì¶©ì „ ê¸°ìˆ ì˜ ì£¼ìš” íŠ¹í—ˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
        "ë°°í„°ë¦¬ ê´€ë ¨ ìµœì‹  íŠ¹í—ˆ ê¸°ìˆ ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "OLED ë””ìŠ¤í”Œë ˆì´ íŠ¹í—ˆì˜ ê¸°ìˆ ì  íŠ¹ì§•ì€?",
        "AI ë°˜ë„ì²´ ê´€ë ¨ íŠ¹í—ˆ ë™í–¥ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
        "ì „ê¸°ì°¨ ë°°í„°ë¦¬ ê´€ë¦¬ ì‹œìŠ¤í…œ íŠ¹í—ˆëŠ”?",
    ]

    query_input_method = st.radio("ì§ˆì˜ ì…ë ¥ ë°©ë²•:", ["ì§ì ‘ ì…ë ¥", "ì˜ˆì‹œ ì„ íƒ"])

    if query_input_method == "ì§ì ‘ ì…ë ¥":
        user_query = st.text_area(
            "ì§ˆì˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ì˜ˆ: ë¬´ì„ ì¶©ì „ ê¸°ìˆ ì˜ ì£¼ìš” íŠ¹í—ˆëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            height=100,
        )
    else:
        user_query = st.selectbox("ì˜ˆì‹œ ì§ˆì˜ ì„ íƒ:", [""] + example_queries)

    # Process Query
    if st.button("ğŸ¯ CRAG ë¶„ì„ ì‹¤í–‰", disabled=not user_query.strip()):
        if not st.session_state.get("crag_pipeline"):
            st.error("CRAG Pipelineì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        with st.spinner("LangGraph CRAG Pipeline ì‹¤í–‰ ì¤‘..."):
            try:
                # Process query through CRAG Pipeline
                result = st.session_state.crag_pipeline.process_query(
                    user_query.strip()
                )

                # Store result in session state
                st.session_state.last_crag_result = result

                # Display Results
                if result["success"]:
                    st.success("âœ… CRAG ë¶„ì„ ì™„ë£Œ!")

                    # Metadata display
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        retry_icon = (
                            "ğŸ”„"
                            if result["metadata"].get("retry_count", 0) > 0
                            else "âœ…"
                        )
                        st.metric(
                            "ì¬ì‹œë„ íšŸìˆ˜",
                            f"{retry_icon} {result['metadata'].get('retry_count', 0)}",
                        )
                    with col2:
                        rewrite_icon = (
                            "âœï¸"
                            if result["metadata"].get("query_rewritten", False)
                            else "ğŸ“"
                        )
                        st.metric(
                            "ì§ˆë¬¸ ì¬ì‘ì„±",
                            f"{rewrite_icon} {'ì˜ˆ' if result['metadata'].get('query_rewritten', False) else 'ì•„ë‹ˆì˜¤'}",
                        )
                    with col3:
                        st.metric(
                            "íŠ¹í—ˆ ë¬¸ì„œ ìˆ˜",
                            f"ğŸ“„ {result['metadata'].get('documents_found', 0)}",
                        )
                    with col4:
                        # ê²€ìƒ‰ ë°©ì‹ í‘œì‹œ
                        search_method = (
                            "ğŸ” í•˜ì´ë¸Œë¦¬ë“œ"
                            if result["metadata"].get("hybrid_search_used", False)
                            else "ğŸ“Š Vector"
                        )
                        rerank_used = result["metadata"].get("reranking_used", False)
                        if rerank_used:
                            search_method += " + ì¬ë­í‚¹"
                        st.metric("ê²€ìƒ‰ ë°©ì‹", search_method)

                    # ê²€ìƒ‰ ì‹œìŠ¤í…œ ì •ë³´ ë°°ì§€
                    search_badges = []
                    if result["metadata"].get("hybrid_search_used", False):
                        search_badges.append("ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰")
                    if result["metadata"].get("reranking_used", False):
                        search_badges.append("ğŸ¯ Cross-encoder ì¬ë­í‚¹")
                    if result["metadata"].get("vector_store_only"):
                        search_badges.append("ğŸ›ï¸ Vector Store ì „ìš©")

                    if search_badges:
                        st.info(" + ".join(search_badges))

                    # Final Answer
                    st.subheader("ğŸ’¡ ìƒì„±ëœ ë‹µë³€")
                    with st.expander("ë‹µë³€ ë‚´ìš©", expanded=True):
                        st.markdown(result["answer"])

                    # Process Log
                    st.subheader("ğŸ” ì²˜ë¦¬ ê³¼ì •")
                    with st.expander("ìƒì„¸ ë¡œê·¸", expanded=False):
                        for i, log_entry in enumerate(result["process_log"], 1):
                            st.markdown(f"**{i}.** {log_entry}")

                    # Retrieved Documents
                    if result["documents"]:
                        st.subheader("ğŸ“š ê²€ìƒ‰ëœ íŠ¹í—ˆ ë¬¸ì„œ")

                        # ì¬ë­í‚¹ ì •ë³´ ìš”ì•½
                        has_rerank_scores = any(
                            doc.get("rerank_score") is not None
                            for doc in result["documents"]
                        )
                        if has_rerank_scores:
                            avg_rerank_score = sum(
                                doc.get("rerank_score", 0)
                                for doc in result["documents"]
                            ) / len(result["documents"])
                            max_rerank_score = max(
                                doc.get("rerank_score", 0)
                                for doc in result["documents"]
                            )
                            min_rerank_score = min(
                                doc.get("rerank_score", 0)
                                for doc in result["documents"]
                            )

                            rerank_col1, rerank_col2, rerank_col3 = st.columns(3)
                            with rerank_col1:
                                st.metric(
                                    "í‰ê·  ì¬ë­í‚¹ ìŠ¤ì½”ì–´", f"{avg_rerank_score:.3f}"
                                )
                            with rerank_col2:
                                st.metric("ìµœê³  ìŠ¤ì½”ì–´", f"{max_rerank_score:.3f}")
                            with rerank_col3:
                                st.metric("ìµœì € ìŠ¤ì½”ì–´", f"{min_rerank_score:.3f}")

                        with st.expander(
                            f"íŠ¹í—ˆ ë¬¸ì„œ ëª©ë¡ ({len(result['documents'])}ê°œ)",
                            expanded=False,
                        ):
                            for i, doc in enumerate(result["documents"], 1):
                                with st.container():
                                    # ë¬¸ì„œ ì œëª©ì— ì¬ë­í‚¹ ì •ë³´ í¬í•¨
                                    rerank_score = doc.get("rerank_score")
                                    original_pos = doc.get("original_position")
                                    rerank_pos = doc.get("rerank_position")

                                    title_parts = [f"**íŠ¹í—ˆ ë¬¸ì„œ {i}**"]
                                    if rerank_score is not None:
                                        score_emoji = (
                                            "ğŸ¯"
                                            if rerank_score > 0.7
                                            else "ğŸ”" if rerank_score > 0.3 else "ğŸ“„"
                                        )
                                        title_parts.append(
                                            f"{score_emoji} ê´€ë ¨ë„: {rerank_score:.3f}"
                                        )

                                    if (
                                        original_pos
                                        and rerank_pos
                                        and original_pos != rerank_pos
                                    ):
                                        if original_pos > rerank_pos:
                                            title_parts.append(
                                                f"ğŸ“ˆ ìˆœìœ„ ìƒìŠ¹: {original_pos}â†’{rerank_pos}"
                                            )
                                        else:
                                            title_parts.append(
                                                f"ğŸ“‰ ìˆœìœ„ í•˜ë½: {original_pos}â†’{rerank_pos}"
                                            )

                                    st.markdown(" | ".join(title_parts))

                                    # Document content preview
                                    content = doc.get("content", str(doc))
                                    if isinstance(content, dict):
                                        content = content.get(
                                            "page_content", str(content)
                                        )

                                    if len(content) > 500:
                                        preview = content[:500] + "..."
                                    else:
                                        preview = content

                                    st.text_area(
                                        f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸° {i}",
                                        value=preview,
                                        height=100,
                                        disabled=True,
                                        key=f"doc_preview_{i}",
                                        label_visibility="collapsed",
                                    )

                                    # Document metadata
                                    if isinstance(doc, dict) and "metadata" in doc:
                                        metadata = doc["metadata"]

                                        # íŠ¹í—ˆ ë²ˆí˜¸ì™€ ì„¹ì…˜ ì •ë³´ í‘œì‹œ
                                        meta_col1, meta_col2, meta_col3 = st.columns(3)
                                        with meta_col1:
                                            st.markdown(
                                                f"**íŠ¹í—ˆë²ˆí˜¸:** `{metadata.get('patent_number', 'N/A')}`"
                                            )
                                        with meta_col2:
                                            st.markdown(
                                                f"**ì„¹ì…˜:** `{metadata.get('section_type', 'N/A')}`"
                                            )
                                        with meta_col3:
                                            if rerank_score is not None:
                                                st.markdown(
                                                    f"**ì¬ë­í‚¹ ìŠ¤ì½”ì–´:** `{rerank_score:.3f}`"
                                                )

                                        # ì „ì²´ ë©”íƒ€ë°ì´í„°ëŠ” ì ‘ì„ ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ
                                        with st.expander(
                                            "ì „ì²´ ë©”íƒ€ë°ì´í„°", expanded=False
                                        ):
                                            # ì¬ë­í‚¹ ê´€ë ¨ ì •ë³´ë„ í¬í•¨
                                            full_metadata = metadata.copy()
                                            if rerank_score is not None:
                                                full_metadata["rerank_score"] = (
                                                    rerank_score
                                                )
                                            if original_pos:
                                                full_metadata["original_position"] = (
                                                    original_pos
                                                )
                                            if rerank_pos:
                                                full_metadata["rerank_position"] = (
                                                    rerank_pos
                                                )
                                            st.json(full_metadata)

                                    st.divider()

                else:
                    st.error("âŒ CRAG ë¶„ì„ ì‹¤íŒ¨")
                    st.error(result["answer"])

                    if result.get("process_log"):
                        with st.expander("ì˜¤ë¥˜ ë¡œê·¸", expanded=True):
                            for log_entry in result["process_log"]:
                                st.text(log_entry)

            except Exception as e:
                st.error(f"CRAG Pipeline ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                st.exception(e)


def display_judgement_result(judge_name: str, judgement):
    """Display judgement result in a formatted way"""

    # Score display with color coding
    score_color = (
        "ğŸŸ¢" if judgement.score >= 0.8 else "ğŸŸ¡" if judgement.score >= 0.6 else "ğŸ”´"
    )
    pass_icon = "âœ…" if judgement.pass_threshold else "âŒ"

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ì ìˆ˜", f"{score_color} {judgement.score:.3f}")
    with col2:
        st.metric("ì‹ ë¢°ë„", f"{judgement.confidence:.3f}")
    with col3:
        st.metric("í†µê³¼", f"{pass_icon}")

    # Explanation
    st.markdown("**ì„¤ëª…:**")
    st.info(judgement.explanation)

    # Details specific to each judge type
    if judge_name == "relevance" and judgement.details:
        if judgement.details.get("relevant_docs"):
            st.markdown("**ê´€ë ¨ ë¬¸ì„œ:**")
            st.code(", ".join(judgement.details["relevant_docs"]))
        if judgement.details.get("irrelevant_docs"):
            st.markdown("**ë¬´ê´€í•œ ë¬¸ì„œ:**")
            st.code(", ".join(judgement.details["irrelevant_docs"]))

    elif judge_name == "hallucination" and judgement.details:
        col1, col2 = st.columns(2)
        with col1:
            if judgement.details.get("supported_claims"):
                st.markdown("**ì§€ì›ë˜ëŠ” ì£¼ì¥:**")
                st.success(judgement.details["supported_claims"])
        with col2:
            if judgement.details.get("unsupported_claims"):
                st.markdown("**ì§€ì›ë˜ì§€ ì•ŠëŠ” ì£¼ì¥:**")
                st.warning(judgement.details["unsupported_claims"])

        risk_level = judgement.details.get("hallucination_risk", "ì¤‘ê°„")
        risk_color = {"ë‚®ìŒ": "ğŸŸ¢", "ì¤‘ê°„": "ğŸŸ¡", "ë†’ìŒ": "ğŸ”´"}.get(risk_level, "ğŸŸ¡")
        st.markdown(f"**í• ë£¨ì‹œë„¤ì´ì…˜ ìœ„í—˜ë„:** {risk_color} {risk_level}")

    elif judge_name == "quality" and judgement.details:
        quality_metrics = [
            "completeness",
            "clarity",
            "accuracy",
            "structure",
            "usefulness",
        ]
        metric_names = ["ì™„ì„±ë„", "ëª…í™•ì„±", "ì •í™•ì„±", "êµ¬ì¡°", "ìœ ìš©ì„±"]

        cols = st.columns(len(quality_metrics))
        for i, (metric, name) in enumerate(zip(quality_metrics, metric_names)):
            if metric in judgement.details:
                with cols[i]:
                    value = judgement.details[metric]
                    st.metric(name, f"{value:.2f}")

        if judgement.details.get("improvements"):
            st.markdown("**ê°œì„  ì œì•ˆ:**")
            st.warning(judgement.details["improvements"])


if __name__ == "__main__":
    main()
